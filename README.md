# ğŸ’± Currency Quotes Automation

ğŸš€ **Automatiza a extraÃ§Ã£o de cotaÃ§Ãµes de moedas e salva em um banco de dados com pipelines orquestrados no Apache Airflow.**

---

## ğŸ’¡ Sobre o Projeto

Este projeto foi desenvolvido como estudo prÃ¡tico de **orquestraÃ§Ã£o de pipelines de dados** com **Apache Airflow**, integrando APIs externas (AwesomeAPI) para **coleta automatizada de cotaÃ§Ãµes de moedas** e salvando os dados tratados em bancos **PostgreSQL** ou **SQLite**.  

Ele demonstra conceitos de **ETL/ELT**, **boas prÃ¡ticas com Pydantic**, e **execuÃ§Ã£o distribuÃ­da com PySpark**, totalmente containerizado com **Docker**.

---

## ğŸ§­ Fluxo de Dados

```mermaid
flowchart LR
    A[ğŸŒ AwesomeAPI<br>Coleta de CotaÃ§Ãµes] --> B[âš™ï¸ Airflow<br>Orquestra as DAGs]
    B --> C[ğŸ”¥ PySpark<br>TransformaÃ§Ãµes e Limpeza]
    C --> D[(ğŸ’¾ Banco de Dados<br>PostgreSQL / SQLite)]
```

ğŸ“Š O Airflow agenda e executa DAGs diÃ¡rias para coletar as cotaÃ§Ãµes, processÃ¡-las com PySpark e persistir no banco relacional escolhido.

---

## ğŸ› ï¸ Tecnologias Utilizadas

- ğŸ Python  
- âš¡ PySpark  
- ğŸ§® Pandas  
- ğŸª¶ Apache Airflow  
- ğŸ³ Docker  
- ğŸ§© Pydantic  
- ğŸ˜ PostgreSQL / ğŸª¶ SQLite  

---

## ğŸ³ Executando via Docker

### âš™ï¸ Requisitos
- Docker Desktop instalado  
- `docker-compose` disponÃ­vel  

---

### 1ï¸âƒ£ Clone o repositÃ³rio
```bash
git clone https://github.com/joaobarreto27/study_currency_quotes_1_1
cd study_currency_quotes_1_1
```

---

### 2ï¸âƒ£ Crie o arquivo `.env` na raiz do projeto
```env
API_TOKEN=seu_token_api
AIRFLOW_IMAGE_NAME=apache/airflow:2.5.1
AIRFLOW_UID=50000
```

ğŸ”‘ **Gerando o API Token:**
- Acesse [AwesomeAPI - InstruÃ§Ãµes de API Key](https://docs.awesomeapi.com.br/instrucoes-api-key)
- Gere seu token e preencha a variÃ¡vel `API_TOKEN` no `.env`

---

### 3ï¸âƒ£ Suba os serviÃ§os

```bash
# 1ï¸âƒ£ Buildar tudo (sem cache):
docker-compose build --no-cache

# 2ï¸âƒ£ Subit somente o cotainer init:
docker-compose up airflow-init

# 3ï¸âƒ£ Subir todo o restante do ambiente ambiente:
docker-compose up -d
```

âœ… O Airflow estarÃ¡ disponÃ­vel em:  
ğŸ‘‰ [http://localhost:8080](http://localhost:8080)

Caso seja alterado algo no ambiente docker, basta realizar o comando abaixo:
```bash
docker-compose up -d
```

---

## ğŸ’» Executando Localmente (sem Docker) com Poetry

> âš ï¸ **AtenÃ§Ã£o:** Rodar este projeto fora do Docker exige que vocÃª tenha configurado corretamente o **PySpark** e o **Java (JDK 17+)** no ambiente local.

### ğŸ§° Requisitos
- Python 3.10+  
- Java JDK 17 ou superior  
- Apache Spark (3.3+ recomendado)  
- VariÃ¡veis de ambiente configuradas:  
  - `JAVA_HOME` apontando para o diretÃ³rio do JDK  
  - `SPARK_HOME` apontando para o diretÃ³rio do Spark  
  - Adicionar `SPARK_HOME/bin` ao `PATH`

### âš™ï¸ Passos para configuraÃ§Ã£o manual
```bash
# 1ï¸âƒ£ Instale o Java (Linux / Mac)
sudo apt install openjdk-17-jdk -y

# 2ï¸âƒ£ Baixe e configure o Apache Spark
wget https://downloads.apache.org/spark/spark-3.3.4/spark-3.3.4-bin-hadoop3.tgz
tar -xzf spark-3.3.4-bin-hadoop3.tgz
mv spark-3.3.4-bin-hadoop3 /opt/spark

# Configure variÃ¡veis de ambiente
export SPARK_HOME=/opt/spark
export PATH=$PATH:$SPARK_HOME/bin
export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64

# 3ï¸âƒ£ Instale o Poetry se ainda nÃ£o tiver
curl -sSL https://install.python-poetry.org | python3 -

# 4ï¸âƒ£ Instale dependÃªncias do projeto via Poetry
poetry install

# 5ï¸âƒ£ Ative o ambiente do Poetry
poetry shell

# 6ï¸âƒ£ Execute o pipeline localmente
python dags/etl/src/main.py
```

> ğŸ’¡ **Dica:** Mesmo com tudo configurado, Ã© **altamente recomendado usar Docker** para garantir reprodutibilidade e compatibilidade com o Airflow e PySpark.

---

## ğŸ—‚ï¸ Estrutura do Projeto

```bash
â”œâ”€â”€ dags/  
â”‚   â””â”€â”€ etl/
â”‚       â”œâ”€â”€ dags/                     # DAGs do Airflow
â”‚       â””â”€â”€ src/
â”‚           â”œâ”€â”€ infrastructure/
â”‚           â”‚   â”œâ”€â”€ data/currency_quotes/   # ExtraÃ§Ã£o de cotaÃ§Ãµes (ARS, USD, BTC, etc.)
â”‚           â”‚   â”œâ”€â”€ databases_connection/   # ConexÃµes SQL
â”‚           â”‚   â”œâ”€â”€ market_data/            # Comandos de manipulaÃ§Ã£o de dados
â”‚           â”‚   â”œâ”€â”€ utils/                  # FunÃ§Ãµes auxiliares
â”‚           â”‚   â””â”€â”€ worker/                 # Workers por moeda
â”‚           â””â”€â”€ ...
â”œâ”€â”€ logs/                          # Logs do Airflow
â”œâ”€â”€ plugins/                       # Plugins personalizados
â””â”€â”€ docker-compose.yml
```

---

## ğŸ’¾ Banco de Dados

O projeto utiliza **PostgreSQL** em produÃ§Ã£o, mas tambÃ©m suporta **SQLite** para testes locais.

Exemplo de configuraÃ§Ã£o no cÃ³digo:

```python
USE_SQLITE = True

connection = ConnectionDatabaseSpark(
    sgbd_name="sqlite" if USE_SQLITE else "postgresql",
    environment="prd" if USE_SQLITE else "prd",
    db_name="1.1_study_currency_quotes",
)
```

---

## ğŸ‘¨â€ğŸ’» Autor

Desenvolvido por [**JoÃ£o Barreto**](https://github.com/joaobarreto27) ğŸ’»  
Projeto de estudo em Engenharia de Dados com foco em automaÃ§Ã£o e orquestraÃ§Ã£o de pipelines.

---

## ğŸ“œ LicenÃ§a

Este projeto estÃ¡ licenciado sob a [MIT License](LICENSE).

---
